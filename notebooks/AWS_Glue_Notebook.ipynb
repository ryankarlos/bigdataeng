{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"tags": [],
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql.dataframe import DataFrame\nfrom pyspark.sql.functions import (\n    col,\n    collect_list,\n    count,\n    desc,\n    expr,\n    lag,\n    max,\n    min,\n    round,\n    sum,\n)\nfrom pyspark.sql.types import (\n    StringType,\n    StructField,\n    StructType,\n    TimestampType,\n)\n\nfrom pyspark.sql.window import Window\nfrom awsglue.dynamicframe import DynamicFrame\n\nimport boto3\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\nclient = boto3.client('s3')\n\n\nSESSION_SCHEMA = StructType(\n    [\n        StructField(\"userid\", StringType(), False),\n        StructField(\"timestamp\", TimestampType(), True),\n        StructField(\"artistid\", StringType(), True),\n        StructField(\"artistname\", StringType(), True),\n        StructField(\"trackid\", StringType(), True),\n        StructField(\"trackname\", StringType(), True),\n    ]\n)\n\nS3_PATH=\"s3://lastfm-dataset/user-session-track.tsv\"\nBUCKET=\"lastfm-dataset\"",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\ndef read_session_data(spark) -> DataFrame:\n    \"\"\"\n    Read session and user profile data and join on userid.\n    :param spark: SparkSession\n    :return: Spark DataFrame\n    \"\"\"\n    data = (\n        spark.read.format(\"csv\")\n        .option(\"header\", \"false\")\n        .option(\"delimiter\", \"\\t\")\n        .schema(SESSION_SCHEMA)\n        .load(S3_PATH)\n    )\n    cols_to_drop = (\"artistid\", \"trackid\")\n    return data.drop(*cols_to_drop).cache()\n\n\n\n\ndef create_users_and_distinct_songs_count(df: DataFrame) -> DataFrame:\n    \"\"\"\n    Create a list of user IDs, along with the number of distinct songs each user has played.\n    :param df: Spark DataFrame\n    :return: DataFrame\n    \"\"\"\n    df1 = df.select(\"userid\", \"artistname\", \"trackname\").dropDuplicates()\n    df2 = (\n        df1.groupBy(\"userid\")\n        .agg(count(\"*\").alias(\"DistinctTrackCount\"))\n        .orderBy(desc(\"DistinctTrackCount\"))\n    )\n    return df2\n\n\ndef create_popular_songs(df: DataFrame, limit=100) -> DataFrame:\n    \"\"\"\n    Create a list of the 100 most popular songs (artist and title) in the dataset, with the number of\n    times each was played..\n    :param df: Spark DataFrame\n    :param limit: int\n    :return: DataFrame\n    \"\"\"\n    df1 = (\n        df.groupBy(\"artistname\", \"trackname\")\n        .agg(count(\"*\").alias(\"CountPlayed\"))\n        .orderBy(desc(\"CountPlayed\"))\n        .limit(limit)\n    )\n    return df1\n\n\ndef create_session_ids_for_all_users(\n    df: DataFrame, session_cutoff: int\n) -> DataFrame:\n    \"\"\"\n    Creates a new 'session_ids' for each user depending for a timestamp if time between successive\n    played tracks exceeds session_cutoff.\n    :param df: Spark DataFrame\n    :param session_cutoff: int\n    :return: Spark DataFrame\n    \"\"\"\n    w1 = Window.partitionBy(\"userid\").orderBy(\"timestamp\")\n\n    df1 = (\n        df.withColumn(\"pretimestamp\", lag(\"timestamp\").over(w1))\n        .withColumn(\n            \"delta_mins\",\n            round(\n                (\n                    col(\"timestamp\").cast(\"long\")\n                    - col(\"pretimestamp\").cast(\"long\")\n                )\n                / 60\n            ),\n        )\n        .withColumn(\n            \"sessionflag\",\n            expr(\n                f\"CASE WHEN delta_mins > {session_cutoff} OR delta_mins IS NULL THEN 1 ELSE 0 END\"\n            ),\n        )\n        .withColumn(\"sessionID\", sum(\"sessionflag\").over(w1))\n    )\n    return df1\n\n\ndef compute_top_n_longest_sessions(df: DataFrame, limit: int) -> DataFrame:\n    \"\"\"\n    Calculates the length of each session for each user from difference of timestamps\n    of first and last songs of each session. Returns top n longest sessions, where n is determined\n    by the limit argument.\n    :param df: Spark DataFrame\n    :param limit: int\n    :return: Spark DataFrame\n    \"\"\"\n    df1 = (\n        df.groupBy(\"userid\", \"sessionID\")\n        .agg(\n            min(\"timestamp\").alias(\"session_start_ts\"),\n            max(\"timestamp\").alias(\"session_end_ts\"),\n        )\n        .withColumn(\n            \"session_length(hrs)\",\n            round(\n                (\n                    col(\"session_end_ts\").cast(\"long\")\n                    - col(\"session_start_ts\").cast(\"long\")\n                )\n                / 3600\n            ),\n        )\n        .orderBy(desc(\"session_length(hrs)\"))\n        .limit(limit)\n    )\n    return df1\n\n\ndef longest_sessions_with_tracklist(\n    df: DataFrame, session_cutoff: int = 20, limit: int = 10\n) -> DataFrame:\n    \"\"\"\n    Creates a dataframe of the top 10 longest sessions (by elapsed time), with the following information about\n    each session: userid, timestamp of first and last songs in the  session, and the list of songs played\n    in the session (in order of play). An exsiting session ends if time between successive\n    tracks exceeds session_cutoff.\n    :param df: Spark DataFrame\n    :param session_cutoff:int, Default:20\n    :param limit: int, Default:10\n    :return: DataFrame\n    \"\"\"\n    df1 = create_session_ids_for_all_users(df, session_cutoff)\n    df2 = compute_top_n_longest_sessions(df1, limit)\n    df3 = (\n        df1.join(df2, [\"userid\", \"sessionID\"])\n        .select(\"userid\", \"sessionID\", \"trackname\", \"session_length(hrs)\")\n        .groupBy(\"userid\", \"sessionID\", \"session_length(hrs)\")\n        .agg(collect_list(\"trackname\").alias(\"tracklist\"))\n        .orderBy(desc(\"session_length(hrs)\"))\n    )\n    return df3\n\n\ndef rename_s3_results_key(source_key_prefix, dest_key):\n    #getting all the content/file inside the bucket. \n    response = client.list_objects_v2(Bucket=BUCKET)\n    body = response[\"Contents\"]\n    #Find out the file which has part-000* in it's Key\n    key =  [obj['Key'] for obj in body if source_key_prefix in obj['Key']]\n    client.copy_object(Bucket=BUCKET, CopySource={'Bucket': BUCKET, 'Key': key[0]}, Key=dest_key)\n    client.delete_object(Bucket=BUCKET, Key=key[0])\n    \n\ndef write_ddf_to_s3(df:DataFrame, name: str):\n    dyf = DynamicFrame.fromDF(df.repartition(1), glueContext, name)\n    sink = glueContext.write_dynamic_frame.from_options(frame=dyf, \n                                                        # use s3a as seems to prevent creating '_$folder$' in S3\n                                                        connection_type = \"s3a\",\n                                                        format = \"glueparquet\",\n                                                        connection_options = {\"path\": f\"s3a://{BUCKET}/results/{name}/\", \"partitionKeys\": []},\n                                                        transformation_ctx = f\"{name}_sink\"\n                                                                )\n    source_key_prefix = f\"results/{name}/run-\"\n    dest_key = f\"results/{name}/{name}.parquet\"\n    rename_s3_results_key(source_key_prefix, dest_key)\n    return sink\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 35,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\ndf = read_session_data(spark)\ndf.printSchema()\n\ndf.show(5)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- userid: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- artistname: string (nullable = true)\n |-- trackname: string (nullable = true)\n\n+-----------+-------------------+----------+--------------------+\n|     userid|          timestamp|artistname|           trackname|\n+-----------+-------------------+----------+--------------------+\n|user_000001|2009-05-04 23:08:57| Deep Dish|Fuck Me Im Famous...|\n|user_000001|2009-05-04 13:54:10|  坂本龍一|Composition 0919 ...|\n|user_000001|2009-05-04 13:52:04|  坂本龍一|Mc2 (Live_2009_4_15)|\n|user_000001|2009-05-04 13:42:52|  坂本龍一|Hibari (Live_2009...|\n|user_000001|2009-05-04 13:42:11|  坂本龍一|Mc1 (Live_2009_4_15)|\n+-----------+-------------------+----------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\n\nsongs_per_user = create_users_and_distinct_songs_count(df)\nsongs_per_user.show()\n\n\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 36,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+------------------+\n|     userid|DistinctTrackCount|\n+-----------+------------------+\n|user_000691|             63636|\n|user_000861|             50230|\n|user_000681|             43241|\n|user_000800|             39542|\n|user_000427|             35934|\n|user_000774|             34620|\n|user_000702|             31342|\n|user_000345|             26055|\n|user_000882|             24990|\n|user_000783|             24569|\n|user_000451|             23513|\n|user_000692|             22392|\n|user_000910|             22311|\n|user_000162|             22143|\n|user_000313|             20355|\n|user_000031|             19864|\n|user_000870|             19847|\n|user_000896|             19836|\n|user_000483|             19479|\n|user_000210|             19269|\n+-----------+------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "popular_songs = create_popular_songs(df)\npopular_songs.show()\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 37,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-------------------+--------------------+-----------+\n|         artistname|           trackname|CountPlayed|\n+-------------------+--------------------+-----------+\n| The Postal Service|  Such Great Heights|       3992|\n|       Boy Division|Love Will Tear Us...|       3663|\n|          Radiohead|        Karma Police|       3534|\n|               Muse|Supermassive Blac...|       3483|\n|Death Cab For Cutie|     Soul Meets Body|       3479|\n|          The Knife|          Heartbeats|       3156|\n|               Muse|           Starlight|       3060|\n|        Arcade Fire|    Rebellion (Lies)|       3048|\n|     Britney Spears|          Gimme More|       3004|\n|        The Killers| When You Were Young|       2998|\n|           Interpol|                Evil|       2989|\n|         Kanye West|       Love Lockdown|       2950|\n|     Massive Attack|            Teardrop|       2948|\n|Death Cab For Cutie|I Will Follow You...|       2947|\n|               Muse| Time Is Running Out|       2945|\n|         Bloc Party|             Banquet|       2906|\n|        Arcade Fire|Neighborhood #1 (...|       2826|\n|          Radiohead|          All I Need|       2696|\n| The Postal Service|      Nothing Better|       2670|\n|        Snow Patrol|        Chasing Cars|       2667|\n+-------------------+--------------------+-----------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "df_sessions = longest_sessions_with_tracklist(df)\ndf_sessions.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 38,
			"outputs": [
				{
					"name": "stdout",
					"text": "+-----------+---------+-------------------+--------------------+\n|     userid|sessionID|session_length(hrs)|           tracklist|\n+-----------+---------+-------------------+--------------------+\n|user_000949|      149|              354.0|[Chained To You, ...|\n|user_000997|       18|              353.0|[Unentitled State...|\n|user_000949|      553|              309.0|[White Daisy Pass...|\n|user_000544|       75|              252.0|[Finally Woken, O...|\n|user_000949|      137|              212.0|[Neighborhood #2 ...|\n|user_000949|      187|              187.0|[Disco Science, H...|\n|user_000949|      123|              187.0|[Excuse Me Miss A...|\n|user_000544|       55|              181.0|[La Murga, Breath...|\n|user_000250|     1258|              174.0|[Lazarus Heart, S...|\n|user_000949|      150|              170.0|[Y-Control, Banqu...|\n+-----------+---------+-------------------+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "\nwrite_ddf_to_s3(popular_songs, \"popular_songs\")\nwrite_ddf_to_s3(df_sessions, \"df_sessions\")\nwrite_ddf_to_s3(songs_per_user, \"distinct_songs\")\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 1,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}